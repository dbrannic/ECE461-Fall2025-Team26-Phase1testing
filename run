#!/bin/bash

# ML Model Evaluation System - Run Script
# Usage: ./run [install|test|URL_FILE]

set -e  # Exit on any error

# Function to install dependencies
install_dependencies() {
    echo "Installing ML Model Evaluation System dependencies..." >&2

    # Check if Python is available
    PYTHON_CMD=""
    if command -v py &> /dev/null; then
        PYTHON_CMD="py"
        echo "Using Python command: py" >&2
    elif command -v python3 &> /dev/null; then
        PYTHON_CMD="python3"
        echo "Using Python command: python3" >&2
    elif command -v python &> /dev/null; then
        PYTHON_CMD="python"
        echo "Using Python command: python" >&2
    else
        echo "Error: Python is not installed or not in PATH" >&2
        exit 1
    fi

    # Check if pip is available
    if ! $PYTHON_CMD -m pip --version &> /dev/null; then
        echo "Error: pip is not available. Please install pip first." >&2
        exit 1
    fi

    # Install requirements
    if [ -f "requirements.txt" ]; then
        echo "Installing packages from requirements.txt..." >&2
        
        if $PYTHON_CMD -m pip install -r requirements.txt --user > /dev/null 2>&1; then
            echo "âœ“ All dependencies installed successfully!" >&2
            exit 0
        else
            echo "Error: Failed to install dependencies" >&2
            exit 1
        fi
    else
        echo "Error: requirements.txt not found" >&2
        exit 1
    fi
}

# Function to run tests
run_tests() {
    # Check if Python is available
    PYTHON_CMD=""
    if command -v py &> /dev/null; then
        PYTHON_CMD="py"
    elif command -v python3 &> /dev/null; then
        PYTHON_CMD="python3"
    elif command -v python &> /dev/null; then
        PYTHON_CMD="python"
    else
        echo "Error: Python is not installed" >&2
        exit 1
    fi
    
    # Check if pytest is installed
    if ! $PYTHON_CMD -m pytest --version &> /dev/null; then
        echo "Error: pytest is not installed. Run './run install' first." >&2
        exit 1
    fi
    
    # Run pytest and capture ALL output (stdout+stderr)
    pytest_output=$($PYTHON_CMD -m pytest backend/src/Testing/ \
        --cov=backend/src \
        --cov-report=term \
        --tb=line \
        -q 2>&1)

    # Store exit code
    test_exit_code=$?

    # Print pytest output to stderr for debugging (autograder reads only stdout)
    echo "$pytest_output" >&2

    # === PARSE TEST COUNTS ===
    passed=0
    failed=0
    errors=0
    total=0

    # Prefer the concise summary patterns
    if echo "$pytest_output" | grep -qE "[0-9]+ passed"; then
        passed=$(echo "$pytest_output" | grep -oE "[0-9]+ passed" | tail -1 | grep -oE "[0-9]+")
    fi
    if echo "$pytest_output" | grep -qE "[0-9]+ failed"; then
        failed=$(echo "$pytest_output" | grep -oE "[0-9]+ failed" | tail -1 | grep -oE "[0-9]+")
    fi
    if echo "$pytest_output" | grep -qE "[0-9]+ error"; then
        errors=$(echo "$pytest_output" | grep -oE "[0-9]+ error" | tail -1 | grep -oE "[0-9]+")
    fi

    # Try to pick up the 'collected N items' line if present
    if echo "$pytest_output" | grep -qE "collected [0-9]+"; then
        collected=$(echo "$pytest_output" | grep -oE "collected [0-9]+" | tail -1 | grep -oE "[0-9]+")
        # If we didn't get per-result numbers, assume collected == total
        if [ -z "$passed" ] || [ "$passed" -eq 0 ]; then
            passed=${passed:-0}
        fi
        total=$((collected))
    fi

    # Compute total from parsed numbers if not set by collected
    if [ -z "$total" ] || [ "$total" -eq 0 ]; then
        passed=${passed:-0}
        failed=${failed:-0}
        errors=${errors:-0}
        total=$((passed + failed + errors))
    else
        # we have a collected count; if passed is missing, derive it
        if [ "$passed" -eq 0 ] && [ $total -gt 0 ]; then
            passed=$((total - failed - errors))
        fi
    fi

    # Final safety defaults
    passed=${passed:-0}
    total=${total:-0}

    # === PARSE COVERAGE ===
    coverage=0
    # Look for lines with a percent value (prefer the last % in output)
    cov_candidate=$(echo "$pytest_output" | grep -oE "[0-9]+%" | tail -1 | tr -d '%') || true
    if [ -n "$cov_candidate" ]; then
        coverage=$cov_candidate
    fi

    coverage=${coverage:-0}

    # Ensure numeric defaults
    if [ -z "$passed" ]; then passed=0; fi
    if [ -z "$total" ]; then total=0; fi
    if [ -z "$coverage" ]; then coverage=0; fi

    # === CRITICAL: OUTPUT TO STDOUT IN EXACT FORMAT ===
    # The autograder parses this single stdout line
    echo "${passed}/${total} test cases passed. ${coverage}% line coverage achieved."

    # Exit with pytest's exit code so CI/test harness can see failures
    exit $test_exit_code
}

# Function to run evaluation on URL file
run_evaluation() {
    local url_file="$1"
    
    # Check if file exists
    if [ ! -f "$url_file" ]; then
        echo "Error: URL file not found: $url_file" >&2
        exit 1
    fi
    
    # Check if Python is available
    PYTHON_CMD="python3"
    if ! command -v python3 &> /dev/null; then
        PYTHON_CMD="python"
    fi
    
    if ! command -v $PYTHON_CMD &> /dev/null; then
        echo "Error: Python is not installed" >&2
        exit 1
    fi
    
    # Run evaluation from backend directory
    cd backend
    $PYTHON_CMD src/main.py "$url_file"
    exit_code=$?
    
    exit $exit_code
}

# Function to show usage
show_usage() {
    echo "Usage: ./run [install|test|URL_FILE]" >&2
    echo "" >&2
    echo "Commands:" >&2
    echo "  install     Install all required dependencies" >&2
    echo "  test        Run the test suite" >&2
    echo "  URL_FILE    Run evaluation on URLs in the specified file" >&2
}

# Main script logic
main() {
    if [ $# -eq 0 ]; then
        echo "Error: No command specified" >&2
        show_usage
        exit 1
    fi
    
    local command="$1"
    
    case "$command" in
        "install")
            install_dependencies
            ;;
        "test")
            run_tests
            ;;
        "-h"|"--help"|"help")
            show_usage
            exit 0
            ;;
        *)
            # Assume it's a URL file
            run_evaluation "$command"
            ;;
    esac
}

# Run main function
main "$@"
```

**To use this:**

1. Save this entire script to a file named `run` (no extension) in your project root
2. Make it executable: `chmod +x run`
3. Test it: `./run test`

The expected output should be exactly:
```
187/187 test cases passed. 85% line coverage achieved.